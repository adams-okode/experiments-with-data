{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sense of MPESA transaction statement data over multiple years\n",
    "\n",
    "#### Goal \n",
    "\n",
    " - Extract data from PDF statements generated via MPESA App\n",
    " - Categorize each transaction \n",
    "\n",
    "#### Resources:\n",
    "- [Better Programming](https://betterprogramming.pub/convert-tables-from-pdfs-to-pandas-with-python-d74f8ac31dc2)\n",
    "- [Towards Data Science](https://towardsdatascience.com/how-to-extract-tables-from-pdf-using-python-pandas-and-tabula-py-c65e43bd754)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do :\n",
    "\n",
    "##### Remaining PDFs\n",
    "- [X] Extract CSV from remaining PDFs\n",
    "- [X] Combine all PDF data into a single DF for further analysis\n",
    "\n",
    "##### Further clean up\n",
    " - [X] drop transaction status\n",
    " - [X] balance\n",
    "\n",
    "##### Extract specific transactions \n",
    "\n",
    " - [X] Pay bill charges total\n",
    " - [X] Create columns for other charges\n",
    "\n",
    "##### Prepare for labeling and prediction\n",
    "\n",
    " - [X] Split date columns into YYYY, MM, DD, HH, MM, Day of the Week, Weekday vs Weekend\n",
    "\n",
    "##### Natural Language \n",
    "\n",
    " - [ ] Explore approaches, Natural Language models to make sense of the Details code\n",
    " - [ ] To explore Can use Receipt No as key? Is there a pattern to the MPESA codes?\n",
    "\n",
    "\n",
    "##### Other\n",
    " - [ ] PDF In (e.g via email, copy to storage bucket, cloud function to convert to CSV and/or SQL db/warehouse \n",
    " - [ ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import tabula\n",
    "import pandas as pd\n",
    "\n",
    "# For no-code exploration using Bamboo Lib\n",
    "# https://docs.bamboolib.8080labs.com/documentation/how-tos/installation-and-setup/install-bamboolib\n",
    "#import bamboolib as bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Processing a single PDF file at a time [Redundant]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define directory and file names where PDFs are located\n",
    "# Can be customized \n",
    "\n",
    "#directory = \"~/dev/pdf/\"\n",
    "#file = \"20200101_20200630.pdf\"\n",
    "#file_path = directory + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert the first page\n",
    "\n",
    "#list_df = tabula.read_pdf(file_path)\n",
    "\n",
    "# Output is a list of two dataframes  because the first page of MPESA statement has a summary table (list_df[0])  \n",
    "# followed by a detailed table list_df[1].\n",
    "\n",
    "# But we would need to convert the entire table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert the entire document\n",
    "\n",
    "#list_df = tabula.read_pdf(file_path, pages='all')\n",
    "\n",
    "# Output is a list of dataframes. The list is of length N + 1 where N is the number of pages in the PDF \n",
    "# because the first page of MPESA statement has a summary table (list_df[0]) , \n",
    "# followed by a detailed table list_df[1]. Each subsequent page because an dataframe element in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First element\n",
    "#df_summary = list_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rest of the elements are the detailed MPESA transactions \n",
    "\n",
    "#df_detail = pd.concat(list_df[1:len(list_df)],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop last column which has no relevant data. \n",
    "# The remaining data frame now corresponds to the details of transactions\n",
    "\n",
    "#df_detail.drop(df_detail.columns[[7]],axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Clean up - rename \\r to \" \" or \"\" in case of column name 'Withdraw\\rn'\n",
    "\n",
    "#df_detail.replace(to_replace=[r\"\\r\"],value=[\" \"],regex=True, inplace=True)\n",
    "#df_detail.rename(columns = {'Withdraw\\rn':'Withdrawn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_detail.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Files\n",
    "# Define CSV file name for the converted data\n",
    "\n",
    "#file_csv_2020H1 = \"20200101_20200630.csv\"\n",
    "#file_csv = \"mpesa_2020_2021.csv\"\n",
    "#file_wfch_csv = \"wfch_2020_2021.csv\"\n",
    "\n",
    "# Save as CSV\n",
    "#df_detail.to_csv(file_csv_2020H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Processing all PDFs in a directory at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the data frame that will contain the complete set of data after conversion\n",
    "df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to list directory content we need the os package\n",
    "import os\n",
    "\n",
    "#Customize this. Assumes PDFs are not secured or password protected.\n",
    "pdf_dir = (\"/Users/josiah/dev/experiments-with-data/mpesa_pdf_statements/files/\")\n",
    "\n",
    "# Loop through the contents of provided directory, filter PDF, \n",
    "for pdf in os.listdir(pdf_dir):\n",
    "    #Filter PDF (just checks the extension for now)\n",
    "    if pdf.endswith(\".pdf\"):\n",
    "        # read pdf into df\n",
    "        print(\"Processing \",pdf)\n",
    "        df_single = tabula.read_pdf(pdf_dir+pdf, pages='all')\n",
    "        df_single_detail = pd.concat(df_single[1:len(df_single)],ignore_index=True)\n",
    "        df_all = pd.concat([df_single_detail, df_all], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop last column which has no relevant data. The remaining data frame now corresponds to the details of transactions\n",
    "df_all.drop(df_all.columns[[7]],axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - rename \\r to \" \" or \"\" in case of column name 'Withdraw\\rn'\n",
    "\n",
    "df_all.replace(to_replace=[r\"\\r\"],value=[\" \"],regex=True, inplace=True)\n",
    "df_all.rename(columns = {'Withdraw\\rn':'Withdrawn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Wrangling: Format column data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop 'transaction status' and 'balance' as they are not necessary\n",
    "\n",
    "df_all.drop(df_all.columns[[3,6]],axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date / Time field\n",
    "\n",
    "df_all['Completion Time'] = pd.to_datetime(df_all['Completion Time'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert string number value to a float\n",
    "# adapted from https://pbpython.com/pandas_dtypes.html\n",
    "\n",
    "def to_float(val):\n",
    "    \"\"\"\n",
    "    Convert the string number value to a float\n",
    "     - Remove $ if present\n",
    "     - Remove commas\n",
    "     - Convert to float type\n",
    "    \"\"\"\n",
    "    # first check if val is a float\n",
    "    if isinstance(val, float):\n",
    "        return val\n",
    "    else:\n",
    "        new_val = val.replace(',','').replace('$', '').replace(' ','')\n",
    "        return float(new_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Withdrawn' and 'Paid In' column number values from string to float\n",
    "\n",
    "df_all['Withdrawn'] = df_all['Withdrawn'].apply(to_float)\n",
    "df_all['Paid in'] = df_all['Paid in'].apply(to_float)\n",
    "\n",
    "# Convert Receipt No and Details to string\n",
    "\n",
    "df_all['Receipt No'] = df_all['Receipt No'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - More Columns from date + extract various charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for Day, Month, Week, Year, hour\n",
    "df['year'] = pd.DatetimeIndex(df['Completion Time']).year\n",
    "df['month'] = pd.DatetimeIndex(df['Completion Time']).month\n",
    "df['day'] = pd.DatetimeIndex(df['Completion Time']).day\n",
    "df['hour'] = pd.DatetimeIndex(df['Completion Time']).hour\n",
    "\n",
    "# Create a new column for \"day of the week, with Monday=0 and ending with Sunday=6\n",
    "df['dayofweek'] = pd.DatetimeIndex(df['Completion Time']).dayofweek\n",
    "\n",
    "# Create a new column to distinguish weekdays and weekends\n",
    "df['isweekend'] = pd.DatetimeIndex(df['Completion Time']).dayofweek >=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# - input - list of strings of text we want to convert from df row to column, current df.\n",
    "# - Output - DF with new columns added\n",
    "\n",
    "def df_add_new_column(df_current, column_key, column_value,\n",
    "                      column_to_search, list_of_strings):\n",
    "    \"\"\"\n",
    "    add new column or columns to provided dataframe when string in list_of_strings is matched to column_name\n",
    "     - input: dataframe, column_key, column_value to retain , column to search, list of strings to search for a match in column provided,\n",
    "     - output: dataframe with added column or columns. Number of new columns added = length of list provided\n",
    "    \"\"\"\n",
    "    # To add a check - if column_name and list_of_strings are provided.\n",
    "    \n",
    "    for string in list_of_strings:\n",
    "        df_1 = df_current[~df_current[column_to_search].str.contains(string)].reset_index(drop=True)\n",
    "        df_2 = df_current[df_current[column_to_search].str.contains(string)][[column_key,column_value]].reset_index(drop=True)\n",
    "        df_2.rename(columns={column_value:string},inplace=True)\n",
    "        df_current = pd.merge(df_1,df_2, on=column_key, how=\"left\").reset_index(drop=True)\n",
    "        df_current[string] = df_current[string].fillna(0)\n",
    "    return df_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_key = \"Receipt No\"\n",
    "column_value = \"Withdrawn\"\n",
    "column_to_search = \"Details\"\n",
    "list_of_strings = [\"Pay Bill Charge\",\"Pay Merchant Charge\",\"Customer Transfer of Funds Charge\", \"Withdrawal Charge\", \"Customer Send Money To Unregistered User Charge\"]\n",
    "#string = list_of_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrain = df_add_new_column(df,column_key,column_value,column_to_search,list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrain.to_csv(\"pretrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Some Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Dig into \"Details\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
